{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa as bibliotecas e arquivos necessarios\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "from network_architectures import dense_autoencoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   Faz o download do fashion_mnist da base de dados do keras\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "#   Carrega esse dataset, ja separando entre dados de treinamento e dados de teste\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#   Podemos verificar o formato desses arquivos\n",
    "train_images.shape\n",
    "test_images.shape\n",
    "train_labels.shape\n",
    "test_labels.shape\n",
    "\n",
    "#   Segunda parte - processar os dados -----------------------------------------\n",
    "#   Em seguida, precisamos processar esses dados. Nesse caso, o processamento se resume a\n",
    "#   limitar o valor de cada pixel das imagens no intervalo [0,1]. Fazemos isso dividindo por 255\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Terceira parte - Definir o modelo e fazer o treinamento --------------------\n",
    "\n",
    "#   Carregamos a arquitetura do nosso modelo do nosso arquivo de arquiteturas. Isso deixa o codigo\n",
    "#   bem modular e limpo\n",
    "model = dense_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Compilamos o nosso modelo. Compilar significa dizer qual a funcao custo e otimizador vamos utilizar (entre outras coisinhas)\n",
    "\n",
    "#   Nesse caso, o otimizador eh o \"Adam\". A funcao custo eh uma funcao mse\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 784)               101136    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 202,772\n",
      "Trainable params: 202,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Resumo do modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Dessa vez, antes de proseguirmos com o treinamento, vamos criar um callback.\n",
    "#   O callback serve para que o keras faca coisas entre epocas, como por exemplo,\n",
    "#   salvar o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   um learning rate scheduler muda o learning rate de acordo com qual a epoca atual\n",
    "#   nosso learning scheduler simplesmente muda a taxa ao passar de 10 epocas\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.01,\n",
    "    decay_steps=10,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "learning_schedule = keras.callbacks.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este callback faz com que o treinamento acabe mais rapidamente, caso a função custo não esteja mais sendo minimizada\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Para salvarmos a melhor epoca, usamos o callback de ModelCheckpoint\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('model_checkpoint.hdf5',\n",
    "                    monitor='val_loss',\n",
    "                    verbose=2,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Para vizualizarmos melhor o treinamento do modelo, usaremos o Tensorboard\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Para vizualizarmos as saídas da rede durante o treinamento, vamos criar criar um novo callback\n",
    "#   Esse callback salva as imagens\n",
    "\n",
    "n = 10\n",
    "\n",
    "class VizualizarSaidasCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        figure, axis = plt.subplots(n, 4)\n",
    "        plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "        \n",
    "        for i in range(n):\n",
    "            axis[i,0].imshow(train_images[i], cmap='gray')\n",
    "            prediction = np.squeeze(model.predict(np.expand_dims(train_images[i], 0)))\n",
    "            axis[i,1].imshow(prediction, cmap='gray')\n",
    "            \n",
    "            axis[i,2].imshow(test_images[i], cmap='gray')\n",
    "            prediction = np.squeeze(model.predict(np.expand_dims(test_images[i], 0)))\n",
    "            axis[i,3].imshow(prediction, cmap='gray')\n",
    "\n",
    "            for k in range(4):\n",
    "                axis[i,k].set_yticklabels([])\n",
    "                axis[i,k].set_xticklabels([])\n",
    "                axis[i,k].set_xticks([])\n",
    "                axis[i,k].set_yticks([])        \n",
    "     \n",
    "        plt.tick_params(left=False, bottom=False)\n",
    "        plt.savefig(os.path.join('train_visualization','fig_'+str(epoch)))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   criamos o callback para passar para o metodo .fit\n",
    "callback = [learning_schedule, model_checkpoint, tensorboard, earlystop, VizualizarSaidasCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/938 [..............................] - ETA: 7:18 - loss: 0.2310WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_train_batch_end` time: 0.0043s). Check your callbacks.\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0638 - val_loss: 0.0409\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04086, saving model to model_checkpoint.hdf5\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0404 - val_loss: 0.0393\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04086 to 0.03925, saving model to model_checkpoint.hdf5\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0388 - val_loss: 0.0380\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03925 to 0.03799, saving model to model_checkpoint.hdf5\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0381 - val_loss: 0.0380\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03799\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0377 - val_loss: 0.0373\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03799 to 0.03735, saving model to model_checkpoint.hdf5\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0374 - val_loss: 0.0373\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03735 to 0.03727, saving model to model_checkpoint.hdf5\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0371 - val_loss: 0.0371\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03727 to 0.03711, saving model to model_checkpoint.hdf5\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0368 - val_loss: 0.0370\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03711 to 0.03695, saving model to model_checkpoint.hdf5\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0368 - val_loss: 0.0365\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03695 to 0.03655, saving model to model_checkpoint.hdf5\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0367 - val_loss: 0.0365\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03655 to 0.03655, saving model to model_checkpoint.hdf5\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0364 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03655 to 0.03624, saving model to model_checkpoint.hdf5\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0362 - val_loss: 0.0361\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03624 to 0.03612, saving model to model_checkpoint.hdf5\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0361 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.03612\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0361 - val_loss: 0.0364\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.03612\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0361 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.03612\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0361 - val_loss: 0.0363\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.03612\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0360 - val_loss: 0.0360\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03612 to 0.03605, saving model to model_checkpoint.hdf5\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0359 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.03605\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0359 - val_loss: 0.0362\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.03605\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0358 - val_loss: 0.0359\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03605 to 0.03593, saving model to model_checkpoint.hdf5\n",
      "Epoch 21/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0357 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.03593 to 0.03563, saving model to model_checkpoint.hdf5\n",
      "Epoch 22/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0355 - val_loss: 0.0360\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.03563\n",
      "Epoch 23/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0357 - val_loss: 0.0361\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03563\n",
      "Epoch 24/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0355 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03563\n",
      "Epoch 25/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0356 - val_loss: 0.0358\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03563\n",
      "Epoch 26/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0355 - val_loss: 0.0364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03563\n",
      "Epoch 27/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0355 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.03563 to 0.03562, saving model to model_checkpoint.hdf5\n",
      "Epoch 28/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0354 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03562\n",
      "Epoch 29/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.0354 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03562\n",
      "Epoch 30/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0354 - val_loss: 0.0359\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03562\n",
      "Epoch 31/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0354 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.03562\n",
      "Epoch 32/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0353 - val_loss: 0.0358\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.03562\n",
      "Epoch 33/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0353 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.03562\n",
      "Epoch 34/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0353 - val_loss: 0.0358\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.03562\n",
      "Epoch 35/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0352 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.03562 to 0.03539, saving model to model_checkpoint.hdf5\n",
      "Epoch 36/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0351 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.03539\n",
      "Epoch 37/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0350 - val_loss: 0.0355\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.03539\n",
      "Epoch 38/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0351 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.03539 to 0.03520, saving model to model_checkpoint.hdf5\n",
      "Epoch 39/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0352 - val_loss: 0.0358\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.03520\n",
      "Epoch 40/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0352 - val_loss: 0.0360\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.03520\n",
      "Epoch 41/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0349 - val_loss: 0.0358\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.03520\n",
      "Epoch 42/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0351 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.03520\n",
      "Epoch 43/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0350 - val_loss: 0.0358\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.03520\n",
      "Epoch 44/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0350 - val_loss: 0.0360\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.03520\n",
      "Epoch 45/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0351 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.03520\n",
      "Epoch 46/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0350 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.03520\n",
      "Epoch 47/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0350 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.03520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0349 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.03520\n",
      "Epoch 49/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.03520\n",
      "Epoch 50/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0350 - val_loss: 0.0355\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03520\n",
      "Epoch 51/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.03520 to 0.03509, saving model to model_checkpoint.hdf5\n",
      "Epoch 52/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0347 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03509\n",
      "Epoch 53/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0348 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.03509\n",
      "Epoch 54/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0348 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03509\n",
      "Epoch 55/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.03509\n",
      "Epoch 56/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0355\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.03509\n",
      "Epoch 57/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03509\n",
      "Epoch 58/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03509\n",
      "Epoch 59/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0348 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03509\n",
      "Epoch 60/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0347 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03509\n",
      "Epoch 61/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0346 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03509\n",
      "Epoch 62/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0345 - val_loss: 0.0354\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03509\n",
      "Epoch 63/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0347 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03509\n",
      "Epoch 64/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0346 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03509\n",
      "Epoch 65/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0347 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.03509 to 0.03505, saving model to model_checkpoint.hdf5\n",
      "Epoch 66/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0344 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03505\n",
      "Epoch 67/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0345 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03505\n",
      "Epoch 68/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0346 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.03505\n",
      "Epoch 69/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0345 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03505\n",
      "Epoch 70/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0345 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.03505 to 0.03504, saving model to model_checkpoint.hdf5\n",
      "Epoch 71/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0345 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.03504 to 0.03504, saving model to model_checkpoint.hdf5\n",
      "Epoch 72/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0345 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.03504 to 0.03492, saving model to model_checkpoint.hdf5\n",
      "Epoch 73/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0344 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03492\n",
      "Epoch 74/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0345 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03492\n",
      "Epoch 75/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0344 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03492\n",
      "Epoch 76/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0344 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03492\n",
      "Epoch 77/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0342 - val_loss: 0.0353\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03492\n",
      "Epoch 78/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0345 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03492\n",
      "Epoch 79/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0345 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03492\n",
      "Epoch 80/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0345 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03492\n",
      "Epoch 81/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0342 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03492\n",
      "Epoch 82/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0344 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03492\n",
      "Epoch 83/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0343 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03492\n",
      "Epoch 84/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0343 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03492\n",
      "Epoch 85/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0342 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03492\n",
      "Epoch 86/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0342 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03492\n",
      "Epoch 87/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0344 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.03492\n",
      "Epoch 88/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0343 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03492\n",
      "Epoch 89/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0343 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03492\n",
      "Epoch 90/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0344 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03492\n",
      "Epoch 91/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0342 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03492\n",
      "Epoch 92/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0342 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.03492 to 0.03486, saving model to model_checkpoint.hdf5\n",
      "Epoch 93/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0341 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.03486\n",
      "Epoch 94/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03486\n",
      "Epoch 95/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0349\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03486\n",
      "Epoch 96/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0343 - val_loss: 0.0351\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03486\n",
      "Epoch 97/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03486\n",
      "Epoch 98/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0342 - val_loss: 0.0352\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03486\n",
      "Epoch 99/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 0.0339 - val_loss: 0.0350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00099: val_loss did not improve from 0.03486\n",
      "Epoch 100/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0341 - val_loss: 0.0348\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.03486 to 0.03479, saving model to model_checkpoint.hdf5\n"
     ]
    }
   ],
   "source": [
    "#   Agora vamos enfim treinar o modelo. Usamos o metodo \"fit\", passando como parametro nossos dados de treino, teste, e por quantas epocas\n",
    "#   Esse metodo retorna os dados mostrados durante o treinamento, e em geral e interessante salva-los.\n",
    "\n",
    "fit_history = model.fit(train_images, train_images, validation_data=(test_images, test_images), epochs=100, batch_size=64, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eee76e88a745d817\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eee76e88a745d817\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Voce pode vizualizar diversor gráficos e estatísticas sobre o treinamento usando o tensorboard\n",
    "\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Quarta parte - Salvar o modelo e outros dados --------------------\n",
    "#   Neste ponto, nosso modelo ja esta treinado. Em geral, so queremos treinar o modelo uma unica vez, pois demora muito.\n",
    "#   Vamos entao salvar esse modelo, junto com os dados de treinamento, no disco. Assim, se precisarmos usar ele novamente,\n",
    "#   so precisamos carrega-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Vamos primeiro salvar o historico de treinamento. Para isso, criaremos um dataframe no pandas (por ser mais facil de trabalhar assim)\n",
    "fit_history_df = pandas.DataFrame(fit_history.history)\n",
    "\n",
    "#   Com isso podemos salvar esses dados no disco diretamente\n",
    "with open('fit_history.csv', mode='w') as f:\n",
    "    fit_history_df.to_csv(f)\n",
    "#   Reparar que um arquivo csv foi salvo no disco. Da uma olhadinha nesse arquivo depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   Em seguida, vamos salvar o modelo. Para isso, salvamos tanto a arquitetura quanto os pesos calculados.\n",
    "#   A arquitetura do modelo podemos salvar como um arquivo json, com a funcao\n",
    "model_json_string = model.to_json()\n",
    "open('architecture.json', 'w').write(model_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Eh importante notar que estamos salvando a ULTIMA EPOCA do modelo, e que ela nao necessariamente sera a melhor.\n",
    "#   Para salvarmos a melhor epoca do modelo, precisamos configurar um \"callback\" no keras. Faremos isso em outro tutorial\n",
    "\n",
    "#   Agora um arquivo json foi salvo. Da uma olhadinha nesse arquivo tbm\n",
    "#   Ta na hora de salvar os pesos. Isso eh feito com a funcao\n",
    "model.save_weights('model_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Agora esse arquivo .h5 tbm foi salvo no disco. Com isso temos todo o nosso modelo salvo no disco, e nao precisaremos treina-lo novamente\n",
    "#   Nosso arquivo de treinamento acaba por aqui. Agora vamos analisar o modelo no outro arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
